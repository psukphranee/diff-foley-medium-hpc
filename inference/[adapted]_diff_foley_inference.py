# -*- coding: utf-8 -*-
"""[adapted] diff_foley_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kh_AExmvI-t7Rq69-Lo8LaELpGVx6uff

# Diff-Foley: Inference Pipeline.

## Mount Google Drive.
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive')

# %ls

"""## Install Dependencies

There are some packages that require restarting the kernel so put them in different cells
"""

#REQUIRES RESTARTING KERNEL
# !pip install -U openmim

# #REQUIRES RESTART
# !pip install omegaconf #required in Stage 2 LDM

# #REQUIRES KERNEL RESTART
# #FOR STAGE 2
# !pip install pip==23.3.1
# !pip install torchmetrics==0.11.4

# !pip install pip==23.3.1


# !pip install pytorch-lightning==1.7
#REQUIRES KERNEL RESTART
# For stage 2, loading model
# #pytorch lighting https://lightning.ai/docs/pytorch/1.7.7/. Use this version because some deprecated classes in current version.

# !pip install einops

# !mim install mmcv==1.3.4

# #print(torch.__version__)
# !pip install ftfy regex tqdm
# !pip install git+https://github.com/openai/CLIP.git

"""### Uncomment below if first time pulling from HuggingFace once.


"""

#### Uncomment below if first time pulling from HuggingFace once.


#needed to clone this repo
# !git lfs install

# #First, check the "infernece" folder to see if the model was put there in a previous run
# #If it is not, re-download the repo and move the "diff_foley_ckpt" folder into "inference"
# !rm -rf /content/drive/MyDrive/Diff-Foley-Workspace/Diff-Foley

# #This is the pretrained model from Hugging Face. Use git to download the repo and move files to "inference" folder.
# !git clone https://huggingface.co/SimianLuo/Diff-Foley /content/drive/MyDrive/Diff-Foley-Workspace/Diff-Foley

"""
## Run Model"""

# Commented out IPython magic to ensure Python compatibility.
#Consolidate working path so that we can reference out working space


from omegaconf import OmegaConf #needed for loading LDM Model
import os
import torch
import numpy as np
import librosa
import soundfile as sf
from tqdm import tqdm
import sys

from util import instantiate_from_config #looks like this is located in diff_foley/modules
from demo_util import Extract_CAVP_Features

# Set Device:
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
device = torch.device("cuda")
# Default Setting:

fps = 4                                                     #  CAVP default FPS=4, Don't change it.
batch_size = 40   # Don't change it.
cavp_config_path = "./config/Stage1_CAVP.yaml"              #  CAVP Config
cavp_ckpt_path = "./checkpoints/epoch_latest.pt"     #  CAVP Ckpt
os.path.exists(cavp_ckpt_path)

# Initalize CAVP Model:
extract_cavp = Extract_CAVP_Features(fps=fps, batch_size=batch_size, device=device, config_path=cavp_config_path, ckpt_path=cavp_ckpt_path)

"""### 2. Loading Stage2 LDM Model:

# --------------------------------------------------------------------------------------------------
"""


def load_model_from_config(config, ckpt, verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    # if len(m) > 0 and verbose:
    print("missing keys:")
    print(m)
    # if len(u) > 0 and verbose:
    print("unexpected keys:")
    print(u)
    model.cuda()
    model.eval()
    return model

# LDM Config:
ldm_config_path = "./config/Stage2_LDM.yaml"
ldm_ckpt_path = "./checkpoints/last.ckpt" # Panya Feb 12, 2025

LDM_config = OmegaConf.load(ldm_config_path)
CAVP_config = OmegaConf.load(cavp_config_path)

# Loading LDM:
#The load_model_from_config function uses the provided configuration to instantiate and initialize a LatentDiffusion model, \
#including its submodules (UNetModel, AutoencoderKL, and Video_Feat_Encoder_Posembed). The model is loaded with the weights \
# from the checkpoint file and moved to the GPU for inference. The configuration ensures that all components are correctly \
# set up and initialized according to the specified parameters.
print("Loading LDM...")
latent_diffusion_model = load_model_from_config(LDM_config, ldm_ckpt_path, verbose=True)
print("...Finished Loading LDM")

workspace_path = "../"
"""### 3. Data Preprocess"""

# Sample1:
#video_path = os.path.join(workspace_path, "inference", "demo_videos/gun.mp4")
#save_path = os.path.join(workspace_path, "generate_samples/gun")
#tmp_path = os.path.join(workspace_path, "generate_samples/temp_folder")

## Sample2:
video_path = os.path.join(workspace_path, "inference","demo_videos/drum.mp4")
save_path = os.path.join(workspace_path, "generate_samples/drum")
tmp_path = os.path.join(workspace_path, "generate_samples/temp_folder")

## Sample3:
# video_path = os.path.join(workspace_path, "./demo_videos/car.mp4"
# save_path = os.path.join(workspace_path, "./generate_samples/car"
# tmp_path = os.path.join(workspace_path, "./generate_samples/temp_folder"


start_second = 0              # Video start second
truncate_second = 8.2         # Video end = start_second + truncate_second

# Extract Video CAVP Features & New Video Path:
cavp_feats, new_video_path = extract_cavp(video_path, start_second, truncate_second, tmp_path=tmp_path)

cavp_feats.shape

'''
def seed_everything(seed):
    import random, os
    import numpy as np
    import torch
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True
seed_everything(21)

"""### 4. Diff-Foley Generation:"""

from demo_util import inverse_op

sample_num = 4

# Inference Param:
cfg_scale = 4.5      # Classifier-Free Guidance Scale
cg_scale = 50        # Classifier Guidance Scale


steps = 25                # Inference Steps

sampler = "DPM_Solver"    # DPM-Solver Sampler
# sampler = "DDIM"        # DDIM Sampler
# sampler = "PLMS"        # PLMS Sampler


save_path = save_path + "_CFG{}_CG{}_{}_{}_useDG_{}".format(cfg_scale, cg_scale, sampler, steps, use_double_guidance)
os.makedirs(save_path, exist_ok=True)

# Video CAVP Features:
# [4,33,512]
#create tensor of 4 copies. where is the encoder part?
video_feat = torch.from_numpy(cavp_feats).unsqueeze(0).repeat(sample_num, 1, 1).to(device)
print("video_feat.shape: ", video_feat.shape)


# Truncate the Video Cond:
feat_len = video_feat.shape[1]
truncate_len = 32
window_num = feat_len // truncate_len


audio_list = []     # [sample_list1, sample_list2, sample_list3 ....]
for i in tqdm(range(window_num), desc="Window:"):
    start, end = i * truncate_len, (i+1) * truncate_len

    # 1). Get Video Condition Embed:
    #This model (latent_diffusion_model) is loaded from a checkpoint file
    #code for this is in ddpm.py
    #what does this do?
    #looks like it learns and encodes some temporal information into the video
    #and is used for the guidance
    #The line of code extracts a subset of video features and processes them through the get_learned_conditioning method of the latent diffusion model. This method applies some form of encoding or conditioning to the input features, resulting in an embedding that is used for further processing in the model.
    embed_cond_feat = latent_diffusion_model.get_learned_conditioning(video_feat[:, start:end])

    # 2). CFG unconditional Embedding:
    uncond_cond = torch.zeros(embed_cond_feat.shape).to(device)

    # 3). Diffusion Sampling:
    print("Using Double Guidance: {}".format(use_double_guidance))
    if use_double_guidance:
        audio_samples, _ = latent_diffusion_model.sample_log_with_classifier_diff_sampler(embed_cond_feat, origin_cond=video_feat, batch_size=video_feat.shape[0], sampler_name=sampler, ddim_steps=steps, unconditional_guidance_scale=cfg_scale,unconditional_conditioning=uncond_cond,classifier=classifier, classifier_guide_scale=cg_scale)  # Double Guidance
    else:
        audio_samples, _ = latent_diffusion_model.sample_log_diff_sampler(embed_cond_feat, batch_size=sample_num, sampler_name=sampler, ddim_steps=steps, unconditional_guidance_scale=cfg_scale,unconditional_conditioning=uncond_cond)           #  Classifier-Free Guidance

    # 4). Decode Latent:
    audio_samples = latent_diffusion_model.decode_first_stage(audio_samples)
    audio_samples = audio_samples[:, 0, :, :].detach().cpu().numpy()

    # 5). Spectrogram -> Audio:  (Griffin-Lim Algorithm)
    sample_list = []        #    [sample1, sample2, ....]
    for k in tqdm(range(audio_samples.shape[0]), desc="current samples:"):
        sample = inverse_op(audio_samples[k])
        sample_list.append(sample)
    audio_list.append(sample_list)

"""#### 4.(a) Double Guidance Load:"""

# Whether use Double Guidance:
use_double_guidance = True

if use_double_guidance:
    classifier_config_path = os.path.join(workspace_path, "inference", "config/Double_Guidance_Classifier.yaml")
    classifier_ckpt_path = os.path.join(workspace_path, "Diff-Foley", "diff_foley_ckpt/double_guidance_classifier.ckpt")
    classifier_config = OmegaConf.load(classifier_config_path)
    classifier = load_model_from_config(classifier_config, classifier_ckpt_path)

# Save Samples:
path_list = []
for i in range(sample_num):      # sample_num
    current_audio_list = []
    for k in range(window_num):
        current_audio_list.append(audio_list[k][i])
    current_audio = np.concatenate(current_audio_list,0)
    print(current_audio.shape)
    sf.write(os.path.join(save_path, "sample_{}_diff.wav").format(i), current_audio, 16000)
    path_list.append(os.path.join(save_path, "sample_{}_diff.wav").format(i))
print("Gen Success !!")

# Concat The Video and Sound:
import subprocess
src_video_path = new_video_path
for i in range(sample_num):
    gen_audio_path = path_list[i]
    out_path = os.path.join(save_path, "output_{}.mp4".format(i))
    cmd = ["ffmpeg" ,"-i" ,src_video_path,"-i" , gen_audio_path ,"-c:v" ,"copy" ,"-c:a" ,"aac" ,"-strict" ,"experimental", out_path]
    # cmd = ["ffmpeg" ,"-i" ,src_video_path,"-i" , gen_audio_path ,"-c:v" ,"copy" ,"-c:a" ,"mp3" ,"-strict" ,"experimental", out_path]
    subprocess.check_call(cmd)
print("Gen Success !!")

import torch
print(torch.cuda.device_count())
'''